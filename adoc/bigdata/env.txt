1. ssh免密

首先需要打开偏好设置->共享->远程登录，勾选所有用户

然后，终端执行ssh-keygen -t rsa，之后一路enter键

最后，执行cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys用于授权你的公钥到本地可以无需密码实现登录。

执行ssh localhost命令，无异常，设置成功。

2. `https://archive.apache.org/dist/hadoop/common/hadoop-3.2.0/`下载hadoop

`cd ~/Dev/env/hadoop`


3. 修改文件

进入 `env/hadoop/hadoop-3.2.0/etc/hadoop` 目录，进行文件修改。

core-site.xml

```
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:8020</value>
    </property>
    <!--用来指定hadoop运行时产生文件的存放目录自己创建-->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/Users/wangjiawei/Dev/env/hadoop/tmp</value>
    </property>
</configuration>
```


hdfs-site.xml
```
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <!--不是root用户也可以写文件到hdfs-->
    <property>
        <name>dfs.permissions</name>
        <value>false</value>
    </property>
    <!--把路径换成本地的name坐在位置-->
    <property>
        <name>dfs.namenode.name.dir</name>
        <!-- <value>file:/Users/wangjiawei/Dev/env/hadoop/dfs/name</value> -->
        <value>hdfs://localhost:8020/hadoop/dfs/name</value>
    </property>
    <!--在本地新建一个存放hadoop数据的文件夹，然后将路径在这里配置一下-->
    <property>
        <name>dfs.datanode.data.dir</name>
        <!-- <value>file:/Users/wangjiawei/Dev/env/hadoop/dfs/data</value> -->
        <value>hdfs://localhost:8020/hadoop/dfs/data</value>
    </property>
</configuration>
```

mapred-site.xml
```
<configuration>
    <!-- 指定mapreduce运行在yarn上 -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>

    <property>
        <name>mapred.job.tracker</name>
        <value>localhost:9010</value>
    </property>

    <!-- 新添加 -->
    <!-- 下面的路径就是你hadoop distribution directory -->
    <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>HADOOP_MAPRED_HOME=/Users/wangjiawei/Dev/env/hadoop/hadoop-3.2.0/libexec</value>
    </property>

    <property>
        <name>mapreduce.map.env</name>
        <value>HADOOP_MAPRED_HOME=/Users/wangjiawei/Dev/env/hadoop/hadoop-3.2.0/libexec</value>
    </property>

    <property>
        <name>mapreduce.reduce.env</name>
        <value>HADOOP_MAPRED_HOME=/Users/wangjiawei/Dev/env/hadoop/hadoop-3.2.0/libexec</value>
    </property>
</configuration>
```

yarn-site.xml
```
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>

    <property>
        <name>yarn.resourcemanager.address</name>
        <value>localhost:9000</value>
    </property>

    <property>
        <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>
        <value>100</value>
    </property>
</configuration>
```